{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "from os.path import abspath, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"initial and define neruon network\"\"\"\n",
    "\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"activation function\"\"\"\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"single layer feedforward funtcion\"\"\"\n",
    "\n",
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"full layer feedforward funtcion\"\"\"\n",
    "\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"error function\"\"\"\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    m = Y_hat.shape[1]\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()\n",
    "\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"single layer backpropagation function\"\"\"\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"full layer backpropagation funtcion\"\"\"\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "   \n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \"\"\"\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"file load\"\"\"\n",
    "\n",
    "class Loader:\n",
    "\n",
    "    @staticmethod\n",
    "    def load_img(path):\n",
    "        data = []\n",
    "        try:\n",
    "            with open(f\"{path}\", 'r') as txtFile:\n",
    "                for line in txtFile:\n",
    "                    line = line.replace('\\n', '').split(',')\n",
    "                    x = [int(_) for _ in line]\n",
    "                    data.append(np.array(x))\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def load_label(path):\n",
    "        data = []\n",
    "        try:\n",
    "            with open(f\"{path}\", 'r') as txtFile:\n",
    "                for line in txtFile:\n",
    "                    x = line.replace('\\n', '').split(',')\n",
    "                    a = [1,0,0]\n",
    "                    if x[0] == '0':\n",
    "                        a = [1,0,0]\n",
    "                    elif x[0] == '1':\n",
    "                        a = [0,1,0]\n",
    "                    elif x[0] == '2':\n",
    "                        a = [0,0,1]\n",
    "                    data.append(np.array(a))\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def merge(imgFile, labelFile):\n",
    "        return [(x, y) for x, y in zip(imgFile, labelFile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "784\n"
    }
   ],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 784, \"output_dim\": 30, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 30, \"output_dim\": 10, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "fi1e = Loader()\n",
    "X = fi1e.load_img(f'{abspath(\".\")}/data/test_img.txt')\n",
    "print(len(X[0]))\n",
    "#print(len(train_img))\n",
    "Y = fi1e.load_img(f'{abspath(\".\")}/data/test_label.txt')\n",
    "#print(len(train_label))\n",
    "train_data = fi1e.merge(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "    return params_values, cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "shapes (784,30) and (100,784) not aligned: 30 (dim 1) != 100 (dim 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-738134cfc2b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-52089d3226f5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, nn_architecture, epochs, learning_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcashe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cost_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcost_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7bc3f126f229>\u001b[0m in \u001b[0;36mfull_forward_propagation\u001b[0;34m(X, params_values, nn_architecture)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mW_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mb_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mA_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_layer_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactiv_function_curr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-52d397e4d66d>\u001b[0m in \u001b[0;36msingle_layer_forward_propagation\u001b[0;34m(A_prev, W_curr, b_curr, activation)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msingle_layer_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_curr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (784,30) and (100,784) not aligned: 30 (dim 1) != 100 (dim 0)"
     ]
    }
   ],
   "source": [
    "train(X[:100], Y[:100], nn_architecture, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (784,3) (3,784) ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-9aa34552202e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m#print(y, X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-9aa34552202e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, batch_size, epochs, lr)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mz_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdweight\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdweight\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdbias\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdbias\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-9aa34552202e>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(self, y, z_s, a_s)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# delta = dC/dZ  known as error for each layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# insert the last layer error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDerivitiveActivationFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Perform BackPropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (784,3) (3,784) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers = [784 , 30, 10], activations=['sigmoid', 'sigmoid']):\n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights.append(np.random.randn(layers[i+1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i+1], 1))\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        # return the feedforward value for x\n",
    "        a = np.copy(x)\n",
    "        z_s = []\n",
    "        a_s = [a]\n",
    "        for i in range(len(self.weights)):\n",
    "            activation_function = self.getActivationFunction(self.activations[i])\n",
    "            z_s.append(self.weights[i].dot(a) + self.biases[i])\n",
    "            a = activation_function(z_s[-1])\n",
    "            a_s.append(a)\n",
    "        return (z_s, a_s)\n",
    "    def backpropagation(self,y, z_s, a_s):\n",
    "            dw = []  # dC/dW\n",
    "            db = []  # dC/dB\n",
    "            deltas = [None] * len(self.weights)  # delta = dC/dZ  known as error for each layer\n",
    "            # insert the last layer error\n",
    "            deltas[-1] = (((y-a_s[-1]).dot(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1])))\n",
    "            # Perform BackPropagation\n",
    "            for i in reversed(range(len(deltas)-1)):\n",
    "                deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        \n",
    "            #a= [print(d.shape) for d in deltas]\n",
    "            batch_size = y.shape[1]\n",
    "            db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
    "            dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "            # return the derivitives respect to weight matrix and biases\n",
    "            return dw, db\n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "        for e in range(epochs): \n",
    "            i=0\n",
    "            while(i<len(y)):\n",
    "                x_batch = x[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                i = i+batch_size\n",
    "                z_s, a_s = self.feedforward(x_batch)\n",
    "                dw, db = self.backpropagation(y_batch, z_s, a_s)\n",
    "                self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\n",
    "                self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\n",
    "                print(\"loss = {}\".format(np.linalg.norm(a_s[-1]-y_batch) ))\n",
    "    @staticmethod\n",
    "    def getActivationFunction(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return lambda x : np.exp(x)/(1+np.exp(x))\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x : x\n",
    "        elif(name == 'relu'):\n",
    "            def relu(x):\n",
    "                y = np.copy(x)\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: x\n",
    "    \n",
    "    @staticmethod\n",
    "    def getDerivitiveActivationFunction(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            sig = lambda x : np.exp(x)/(1+np.exp(x))\n",
    "            return lambda x :sig(x)*(1-sig(x)) \n",
    "        elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        elif(name == 'relu'):\n",
    "            def relu_diff(x):\n",
    "                y = np.copy(x)\n",
    "                y[y>=0] = 1\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu_diff\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: 1\n",
    "if __name__=='__main__':\n",
    "    import matplotlib.pyplot as plt\n",
    "    fi1e = Loader()\n",
    "    X = fi1e.load_img(f'{abspath(\".\")}/data/test_img.txt')\n",
    "    #print(len(X[0]))\n",
    "    #print(len(train_img))\n",
    "    Y = fi1e.load_label(f'{abspath(\".\")}/data/test_label.txt')\n",
    "    #print(len(train_label))\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(Y)\n",
    "    nn = NeuralNetwork([784, 30, 3],activations=['sigmoid', 'sigmoid'])\n",
    "    \n",
    "    nn.train(X, y, epochs=10000, batch_size=784, lr = 1)\n",
    "    _, a_s = nn.feedforward(X)\n",
    "    #print(y, X)\n",
    "    plt.scatter(X.flatten(), y.flatten())\n",
    "    plt.scatter(X.flatten(), a_s[-1].flatten())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "w1 = [[ 0.1 -0.3]\n [-0.2  0.1]\n [ 0.2 -0.1]]\na0 = [1 0]\nb1 = [ 0.   0.2 -0.1]\nn1 = [0.1 0.  0.1]\n[0.52497919 0.5        0.52497919]\n"
    }
   ],
   "source": [
    "w1 = np.array([[0.1,-0.2,0.2],[-0.3,0.1,-0.1]]).T\n",
    "print(f'w1 = {w1}')\n",
    "a0 = np.array([1,0])\n",
    "print(f'a0 = {a0}')\n",
    "b1 = np.array([0,0.2,-0.1])\n",
    "print(f'b1 = {b1}')\n",
    "n1 = np.dot(w1,a0)+b1\n",
    "print(f'n1 = {n1}')\n",
    "a1 = sigmoid(n1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Process starting ...\n",
      "epochs: 0\n",
      ">> Trainning finished !\n",
      ">> Starting validation ...\n",
      ">> Validation finished !\n",
      ">> Starting predict ...\n",
      ">> Predict finished !\n",
      "\n",
      "###### Details ######\n",
      "1.Data Count\n",
      " -  Trainning data: 6000\n",
      " - Validation data: 2000\n",
      " -    Predict data: 2000\n",
      " -           Total: 10000\n",
      "\n",
      "2.Hidden layer\n",
      " -     Hidden Layer count: 1\n",
      " - Layer#1's Neuron count: 30\n",
      "\n",
      "3.Final epoch: 30\n",
      "4.Final learning rate: 0.1073193819714686%\n",
      "5.Accuracy Rate\n",
      " -       Bias: 89.93% [5396 / 6000]\n",
      " - Validation: 89.15% [1783 / 2000]\n",
      "\n",
      "6.Error Rate\n",
      " -     Bias: 10.07%\n",
      " - Variance: 0.78%\n",
      "\n",
      ">> Process End-Up !\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "'''\n",
    "created at 2020/11/6\n",
    "author: Lishang Chien\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def cross_entropy(a, y):\n",
    "    return a - y\n",
    "\n",
    "class Loader:\n",
    "    \"\"\"define function for loading txt file\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_img(path):\n",
    "        \"\"\"load train_img/test_img txt file\"\"\"\n",
    "        data = []\n",
    "        try:\n",
    "            with open(f\"{abspath('.')}{path}\", 'r') as txtFile:\n",
    "                for line in txtFile:\n",
    "                    line = line.replace('\\n', '').split(',')\n",
    "                    x = [int(_) for _ in line]\n",
    "                    data.append(np.array(x))\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def load_label(path):\n",
    "        \"\"\"load train_label/test_label txt file\"\"\"\n",
    "        data = []\n",
    "        try:\n",
    "            with open(f\"{abspath('.')}{path}\", 'r') as txtFile:\n",
    "                for line in txtFile:\n",
    "                    x = line.replace('\\n', '').split(',')\n",
    "                    a = [0,0,0]\n",
    "                    if x[0] == '0':\n",
    "                        a = [1,0,0]\n",
    "                    elif x[0] == '1':\n",
    "                        a = [0,1,0]\n",
    "                    elif x[0] == '2':\n",
    "                        a = [0,0,1]\n",
    "                    data.append(np.array(a))\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def merge(imgFile, labelFile):\n",
    "        return [(x, y) for x, y in zip(imgFile, labelFile)]\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, activation=sigmoid, activation_prime=sigmoid_prime, cost=cross_entropy):\n",
    "        \"\"\" initial dependency / hyperparamters \"\"\"\n",
    "        # set activation function\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "        self.cost = cost\n",
    "\n",
    "        # set-up\n",
    "        self.weights = []\n",
    "        self.learningRate = 0\n",
    "\n",
    "        # layers = [784,30,3] means \n",
    "        # input layer has 784 neurons\n",
    "        # one hidden layer has 30 neurons\n",
    "        # output layer has 3 neurons\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learningRate=0.5, epochs=500):\n",
    "        \"\"\" do the job \"\"\"\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0])) # add column of ones to X\n",
    "        X = np.concatenate((ones.T, X), axis=1) # add the bias unit to the input layer\n",
    "        self.learningRate = learningRate\n",
    "        errorMeasure_lastTime = 0\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]] # a = x[0]\n",
    "            errorMeasure = 1\n",
    "\n",
    "            # Feedforward part: \n",
    "            #   calculate a[l] to a[L], acivation function using sigmoid\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "\n",
    "            # Backward part: \n",
    "            #   output layer -> delta L\n",
    "            error = self.cost(a[-1], y[i])\n",
    "            deltas = [error]\n",
    "            #   from second to last layer -> delta L-1,L-2,...,1\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            #   reverse deltas to simplify backpropagation's implementation(L -> l)\n",
    "            deltas.reverse()\n",
    "\n",
    "            # Update weights part:\n",
    "            #   using stotistic backpropagation\n",
    "            for l in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[l])\n",
    "                delta = np.atleast_2d(deltas[l])\n",
    "                self.weights[l] -= self.learningRate * np.dot(layer.T, delta)\n",
    "            #errorMeasure = error ** 2 # Mean Square Error(MSE), sum part\n",
    "            self.learningRate *= 0.95\n",
    "            #errorMeasure /= len(X) # Mean Square Error(MSE), 1/N part\n",
    "            #print(errorMeasure)\n",
    "            #if errorMeasure < 0.02:\n",
    "            #    return k # return result when error measure enough small or error measure stop beening smaller\n",
    "            #errorMeasure_lastTime = errorMeasure\n",
    "\n",
    "            if k % 100 ==0:print('epochs:', k)\n",
    "        return epochs\n",
    "\n",
    "    def predict(self, x): \n",
    "        \"\"\" predict unknown data(a) from well-trainned weights \"\"\"\n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "        for l in range(len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "\n",
    "        return a\n",
    "\n",
    "    def get_accuracy(self, X, Y):\n",
    "        \"\"\" calculate accuracy rate and accuracy number \"\"\"\n",
    "        signal = []\n",
    "        for x,y in zip(X, Y):\n",
    "            result = list(nn.predict(x))\n",
    "            predictNumber = result.index(max(result))\n",
    "            answerNumber = list(y).index(max(y))\n",
    "            signal.append(1 if answerNumber==predictNumber else 0)\n",
    "\n",
    "        accuracyRate = round(signal.count(1)/(len(signal))*100,2)\n",
    "        accuracyNum = f'{signal.count(1)} / {len(signal)}'\n",
    "        return accuracyRate, accuracyNum\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # loading data\n",
    "    fi1e = Loader()\n",
    "    X = fi1e.load_img('/data/train_img.txt')\n",
    "    Y = fi1e.load_label('/data/train_label.txt')\n",
    "    unknown_X = fi1e.load_img('/data/test_img.txt')\n",
    "\n",
    "    # normalization & grouping\n",
    "    train_X = np.array(X[:6000])\n",
    "    train_Y = np.array(Y[:6000])\n",
    "    validation_X = np.array(X[6000:])\n",
    "    validation_Y = np.array(Y[6000:])\n",
    "\n",
    "    # neural network create\n",
    "    nnStructure = [784,30,3]\n",
    "    nn = NeuralNetwork(nnStructure)\n",
    "\n",
    "    # trainning process\n",
    "    print(f'>> Process starting ...')\n",
    "    epochFinal = nn.fit(train_X,\n",
    "                        train_Y, \n",
    "                        learningRate=0.5, \n",
    "                        epochs=30)\n",
    "    print(f'>> Trainning finished !')\n",
    "\n",
    "    # valadation process\n",
    "    print(f'>> Starting validation ...')\n",
    "    accRateT, accNumT = nn.get_accuracy(train_X, train_Y)\n",
    "    accRateV, accNumV = nn.get_accuracy(validation_X, validation_Y)\n",
    "    print(f'>> Validation finished !')\n",
    "\n",
    "    # predict process\n",
    "    print(f'>> Starting predict ...')\n",
    "    result = []\n",
    "    for x in unknown_X:\n",
    "        output = list(nn.predict(x))\n",
    "        predict_number = output.index(max(output))\n",
    "        result.append(predict_number)\n",
    "    print(f'>> Predict finished !')\n",
    "    #print(result)\n",
    "\n",
    "    print(f'')\n",
    "    print(f'###### Details ######')\n",
    "    print(f'1.Data Count')\n",
    "    print(f' -  Trainning data: {len(train_X)}')\n",
    "    print(f' - Validation data: {len(validation_X)}')\n",
    "    print(f' -    Predict data: {len(unknown_X)}')\n",
    "    print(f' -           Total: {len(train_X)+len(validation_X)+len(unknown_X)}')\n",
    "    print(f'')\n",
    "    print(f'2.Hidden layer')\n",
    "    print(f' -     Hidden Layer count: {len(nnStructure)-2}')\n",
    "    for cnt in range(1,len(nnStructure)-1):\n",
    "        print(f' - Layer#{cnt}\\'s Neuron count: {nnStructure[cnt]}')\n",
    "    print(f'')\n",
    "    print(f'3.Final epoch: {epochFinal}')\n",
    "    print(f'4.Final learning rate: {nn.learningRate}%')\n",
    "    print(f'5.Accuracy Rate')\n",
    "    print(f' -       Bias: {accRateT}% [{accNumT}]')\n",
    "    print(f' - Validation: {accRateV}% [{accNumV}]')\n",
    "    print(f'')\n",
    "    print(f'6.Error Rate')\n",
    "    print(f' -     Bias: {round(100-accRateT, 2)}%')\n",
    "    print(f' - Variance: {round(-accRateV+accRateT, 2)}%')\n",
    "    print(f'')\n",
    "    print(f'>> Process End-Up !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.20749376 0.00499584]\n"
    }
   ],
   "source": [
    "w2 = np.array([[0.1,0.3],[-0.1,0],[0.2,-0.1]]).T\n",
    "b2 = np.array([0.1,-0.1])\n",
    "n2 = np.dot(w2,a1)+b2\n",
    "print(n2)\n",
    "a2 = sigmoid(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1.])"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "np.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 0.85081511  0.3015119  -0.76293434]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.87154484  0.29919285 -0.78330211]\n [ 0.84737808  0.30177853 -0.75981105]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.93442471  0.38489885 -0.95013329]\n [ 0.84737803  0.30177854 -0.75981101]\n [ 0.84737803  0.30177854 -0.75981101]]\n"
    }
   ],
   "source": [
    "a = np.array([[0.85081511,0.84737803,0.84737803,0.93442471,0.93442471,0.93442471,\n",
    "  0.84737803,0.93442471, 0.93442471, 0.84737803, 0.84737803, 0.84737803,\n",
    "  0.84737803, 0.93442471, 0.84737803, 0.84737803, 0.87154484, 0.84737808,\n",
    "  0.93442471, 0.84737803, 0.84737803, 0.93442471, 0.84737803, 0.93442471,\n",
    "  0.84737803, 0.84737803, 0.93442471, 0.93442471, 0.84737803, 0.84737803],\n",
    " [0.3015119 , 0.30177854, 0.30177854, 0.38489885, 0.38489885, 0.38489885,\n",
    "  0.30177854, 0.38489885, 0.38489885, 0.30177854, 0.30177854, 0.30177854,\n",
    "  0.30177854, 0.38489885, 0.30177854, 0.30177854, 0.29919285, 0.30177853,\n",
    "  0.38489885, 0.30177854, 0.30177854, 0.38489885, 0.30177854, 0.38489885,\n",
    "  0.30177854, 0.30177854, 0.38489885, 0.38489885, 0.30177854, 0.30177854],\n",
    " [0.23706566, 0.24018899, 0.24018899, 0.04986671, 0.04986671, 0.04986671,\n",
    "  0.24018899, 0.04986671, 0.04986671, 0.24018899 ,0.24018899, 0.24018899,\n",
    "  0.24018899, 0.04986671, 0.24018899, 0.24018899 ,0.21669789, 0.24018895,\n",
    "  0.04986671, 0.24018899, 0.24018899, 0.04986671 ,0.24018899, 0.04986671,\n",
    "  0.24018899, 0.24018899, 0.04986671, 0.04986671 ,0.24018899, 0.24018899]]).T\n",
    "y = np.array([0,0,1])\n",
    "print(a-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "仇(qiu2)乾(qian2)單(shan4)\n"
     ]
    }
   ],
   "source": [
    "convert_dict = {'柏':'(bo2)', '乾':'(qian2)', '粘':'(nian2)', '仇':'(qiu2)', '單':'(shan4)'}\n",
    "name = '仇乾單'\n",
    "name_split = []\n",
    "for i,n in enumerate(name):\n",
    "    cell = n\n",
    "    if n in convert_dict:\n",
    "        cell += convert_dict[n]\n",
    "    name_split.append(cell)\n",
    "print(''.join(name_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '柏乾粘仇單'\n",
    "name_split = name.split()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda7af9ca22254b43f1852c7d3c10f43431",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}